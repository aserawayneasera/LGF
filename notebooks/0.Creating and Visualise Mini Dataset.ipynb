{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the dataset with small, medium and large objects\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "def load_annotations(annotation_file):\n",
    "    with open(annotation_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def count_class_instances(coco, category_ids):\n",
    "    # Count instances for each category ID\n",
    "    class_counts = {cat_id: 0 for cat_id in category_ids}\n",
    "    for ann in coco.anns.values():\n",
    "        cat_id = ann['category_id']\n",
    "        if cat_id in class_counts:\n",
    "            class_counts[cat_id] += 1\n",
    "    return class_counts\n",
    "\n",
    "def count_images_per_class(coco, category_ids):\n",
    "    # Count images per class\n",
    "    image_counts = {cat_id: set() for cat_id in category_ids}\n",
    "    for ann in coco.anns.values():\n",
    "        cat_id = ann['category_id']\n",
    "        image_id = ann['image_id']\n",
    "        if cat_id in image_counts:\n",
    "            image_counts[cat_id].add(image_id)\n",
    "    # Convert sets to counts\n",
    "    return {cat_id: len(images) for cat_id, images in image_counts.items()}\n",
    "\n",
    "def count_object_sizes(coco):\n",
    "    # Count small, medium, and large objects\n",
    "    small, medium, large = 0, 0, 0\n",
    "    for ann in coco.anns.values():\n",
    "        _, _, width, height = ann['bbox']\n",
    "        area = width * height\n",
    "\n",
    "        if area < 32**2:\n",
    "            small += 1\n",
    "        elif 32**2 <= area < 96**2:\n",
    "            medium += 1\n",
    "        else:\n",
    "            large += 1\n",
    "\n",
    "    return small, medium, large\n",
    "\n",
    "def plot_class_distribution(class_counts, coco):\n",
    "    # Convert category IDs to names for plotting\n",
    "    category_names = [coco.cats[cat_id]['name'] for cat_id in class_counts.keys()]\n",
    "    counts = list(class_counts.values())\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(category_names, counts, color='skyblue')\n",
    "    plt.xlabel('Number of Instances')\n",
    "    plt.title('Class Distribution')\n",
    "    plt.show()\n",
    "\n",
    "def plot_size_distribution(size_counts, dataset_type):\n",
    "    sizes = ['Small', 'Medium', 'Large']\n",
    "    counts = size_counts\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(sizes, counts, color=['lightcoral', 'gold', 'lightgreen'])\n",
    "    plt.xlabel('Object Size')\n",
    "    plt.ylabel('Number of Objects')\n",
    "    plt.title(f'Object Size Distribution ({dataset_type} Dataset)')\n",
    "    plt.show()\n",
    "\n",
    "# Paths to your annotation files\n",
    "# train_annotation_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_train2017-8k-5c.json'\n",
    "# val_annotation_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_val2017-2k-5c.json'\n",
    "\n",
    "# train_annotation_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/non_weather-mini/annotations/mini_train2017_non_weather-2400k6c.json'\n",
    "# val_annotation_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/non_weather-mini/annotations/mini_val2017_non_weather-500k6c.json'\n",
    "\n",
    "# train_annotation_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/weather-mini/annotations/mini_train2017_weather-2400k6c.json'\n",
    "# val_annotation_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/weather-mini/annotations/mini_val2017_weather-500k6c.json'\n",
    "train_annotation_file = '/nas.dbms/asera/PROJECTS/DATASET/ACDC-1/ACDC-1-NEW/annotations/mini_train.json'\n",
    "val_annotation_file = '/nas.dbms/asera/PROJECTS/DATASET/ACDC-1/ACDC-1-NEW/annotations/mini_val.json'\n",
    "\n",
    "\n",
    "# Initialize COCO API for both train and validation sets\n",
    "coco_train = COCO(train_annotation_file)\n",
    "coco_val = COCO(val_annotation_file)\n",
    "\n",
    "# Get category IDs from the training dataset\n",
    "category_ids = coco_train.getCatIds()\n",
    "\n",
    "# Count instances for training and validation sets\n",
    "train_class_counts = count_class_instances(coco_train, category_ids)\n",
    "val_class_counts = count_class_instances(coco_val, category_ids)\n",
    "\n",
    "# Count images per class for training and validation sets\n",
    "train_image_counts = count_images_per_class(coco_train, category_ids)\n",
    "val_image_counts = count_images_per_class(coco_val, category_ids)\n",
    "\n",
    "# Count object sizes for training and validation sets\n",
    "train_size_counts = count_object_sizes(coco_train)\n",
    "val_size_counts = count_object_sizes(coco_val)\n",
    "\n",
    "# Total image count\n",
    "total_train_images = len(coco_train.imgs)\n",
    "total_val_images = len(coco_val.imgs)\n",
    "\n",
    "# Plot distributions for training and validation sets\n",
    "print(\"Training Set Class Distribution:\")\n",
    "plot_class_distribution(train_class_counts, coco_train)\n",
    "print(\"Training Set Object Size Distribution:\")\n",
    "plot_size_distribution(train_size_counts, \"Training\")\n",
    "\n",
    "print(\"Validation Set Class Distribution:\")\n",
    "plot_class_distribution(val_class_counts, coco_val)\n",
    "print(\"Validation Set Object Size Distribution:\")\n",
    "plot_size_distribution(val_size_counts, \"Validation\")\n",
    "\n",
    "# Print counts for object sizes\n",
    "print(\"\\nTraining Set Object Size Counts:\")\n",
    "print(f\"Small: {train_size_counts[0]}, Medium: {train_size_counts[1]}, Large: {train_size_counts[2]}\")\n",
    "\n",
    "print(\"\\nValidation Set Object Size Counts:\")\n",
    "print(f\"Small: {val_size_counts[0]}, Medium: {val_size_counts[1]}, Large: {val_size_counts[2]}\")\n",
    "\n",
    "# Print counts for images per class\n",
    "print(\"\\nTraining Set Images per Class:\")\n",
    "for cat_id, count in train_image_counts.items():\n",
    "    print(f\"{coco_train.cats[cat_id]['name']}: {count}\")\n",
    "\n",
    "print(\"\\nValidation Set Images per Class:\")\n",
    "for cat_id, count in val_image_counts.items():\n",
    "    print(f\"{coco_val.cats[cat_id]['name']}: {count}\")\n",
    "\n",
    "# Print total image counts\n",
    "print(f\"\\nTotal Training Images: {total_train_images}\")\n",
    "print(f\"Total Validation Images: {total_val_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training dataset ONLY Randomly selected\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Path to the original COCO annotations\n",
    "original_ann_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/annotations/instances_train2017.json'\n",
    "\n",
    "\n",
    "# # MacBook Air\n",
    "# original_ann_file = '/mnt/localssd/coco2017/annotations/instances_train2017.json'\n",
    "\n",
    "# Parameters for customization\n",
    "num_images = 4000  # Specify the number of images for training (e.g., 100 images)\n",
    "selected_classes = ['bicycle', 'car', 'motorcycle', 'bus', 'truck']  # Specify category names to include, leave empty for all classes (e.g., ['person', 'car'])\n",
    "\n",
    "\n",
    "# Load the original annotations\n",
    "with open(original_ann_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Filter images and annotations by selected classes if specified\n",
    "if selected_classes:\n",
    "    # Get category IDs for the selected classes\n",
    "    category_ids = {cat['id'] for cat in coco_data['categories'] if cat['name'] in selected_classes}\n",
    "\n",
    "    # Filter annotations by category IDs\n",
    "    filtered_annotations = [ann for ann in coco_data['annotations'] if ann['category_id'] in category_ids]\n",
    "\n",
    "    # Get image IDs for the filtered annotations\n",
    "    image_ids = {ann['image_id'] for ann in filtered_annotations}\n",
    "\n",
    "    # Filter images based on these image IDs\n",
    "    filtered_images = [img for img in coco_data['images'] if img['id'] in image_ids]\n",
    "else:\n",
    "    # Use all images and annotations if no classes are specified\n",
    "    filtered_images = coco_data['images']\n",
    "    filtered_annotations = coco_data['annotations']\n",
    "\n",
    "# Shuffle the filtered images randomly\n",
    "random.shuffle(filtered_images)\n",
    "\n",
    "# Limit the number of images to the specified value\n",
    "train_images = filtered_images[:num_images]\n",
    "\n",
    "# Filter annotations for the selected images\n",
    "def filter_annotations(images, annotations):\n",
    "    image_ids = {img['id'] for img in images}\n",
    "    return [ann for ann in annotations if ann['image_id'] in image_ids]\n",
    "\n",
    "train_annotations = filter_annotations(train_images, filtered_annotations)\n",
    "\n",
    "# Create the new JSON structure for training\n",
    "train_data = {\n",
    "    'images': train_images,\n",
    "    'annotations': train_annotations,\n",
    "    'categories': coco_data['categories']\n",
    "}\n",
    "\n",
    "# Save the new JSON file\n",
    "output_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_train2017-4k-5c.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "print(f\"Training images created with {len(train_images)} images.\")\n",
    "if selected_classes:\n",
    "    print(f\"Filtered by classes: {selected_classes}\")\n",
    "else:\n",
    "    print(\"All classes included.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW\n",
    "import json, random, os\n",
    "\n",
    "src = '/nas.dbms/asera/PROJECTS/DATASET/COCO/annotations/instances_train2017.json'\n",
    "dst = '/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_train2017-4k-5c.json'\n",
    "\n",
    "num_images = 4000\n",
    "keep_names = ['bicycle', 'car', 'motorcycle', 'bus', 'truck']\n",
    "\n",
    "data = json.load(open(src))\n",
    "\n",
    "# 1) keep only selected categories and build a contiguous id map 1..K\n",
    "keep_cats = [c for c in data['categories'] if c['name'] in keep_names]\n",
    "keep_cats = sorted(keep_cats, key=lambda c: keep_names.index(c['name']))\n",
    "old2new = {c['id']: i+1 for i, c in enumerate(keep_cats)}\n",
    "new_categories = [\n",
    "    {'id': i+1, 'name': c['name'], 'supercategory': c.get('supercategory','none')}\n",
    "    for i, c in enumerate(keep_cats)\n",
    "]\n",
    "\n",
    "# 2) keep only annotations in those categories, remap category_id\n",
    "anns = [a for a in data['annotations'] if a['category_id'] in old2new]\n",
    "for a in anns:\n",
    "    a['category_id'] = old2new[a['category_id']]\n",
    "\n",
    "# 3) keep only images that have at least one kept annotation\n",
    "keep_img_ids = {a['image_id'] for a in anns}\n",
    "imgs = [im for im in data['images'] if im['id'] in keep_img_ids]\n",
    "\n",
    "# 4) randomly sample images, then restrict annotations to them\n",
    "random.shuffle(imgs)\n",
    "imgs = imgs[:num_images]\n",
    "img_set = {im['id'] for im in imgs}\n",
    "anns = [a for a in anns if a['image_id'] in img_set]\n",
    "\n",
    "# 5) write out clean file\n",
    "out = {\n",
    "    'info': {'description': 'COCO-5 train subset', 'version': '1.0'},\n",
    "    'images': imgs,\n",
    "    'annotations': anns,\n",
    "    'categories': new_categories\n",
    "}\n",
    "json.dump(out, open(dst,'w'))\n",
    "print(f'wrote {dst}: {len(imgs)} images, {len(anns)} anns, {len(new_categories)} cats')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation dataset ONLY Randomly selected\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Path to the original COCO annotations\n",
    "original_ann_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/annotations/instances_val2017.json'\n",
    "\n",
    "# # MacBook Air\n",
    "# original_ann_file = '/mnt/localssd/coco2017/annotations/instances_val2017.json'\n",
    "\n",
    "\n",
    "# Parameters for customization\n",
    "num_images = 2000  # Specify the number of images for validation (e.g., 100 images)\n",
    "selected_classes = ['bicycle', 'car', 'motorcycle', 'bus', 'truck']  # Specify category names to include, leave empty for all classes (e.g., ['person', 'car'])\n",
    "\n",
    "# Load the original annotations\n",
    "with open(original_ann_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Filter images and annotations by selected classes if specified\n",
    "if selected_classes:\n",
    "    # Get category IDs for the selected classes\n",
    "    category_ids = {cat['id'] for cat in coco_data['categories'] if cat['name'] in selected_classes}\n",
    "\n",
    "    # Filter annotations by category IDs\n",
    "    filtered_annotations = [ann for ann in coco_data['annotations'] if ann['category_id'] in category_ids]\n",
    "\n",
    "    # Get image IDs for the filtered annotations\n",
    "    image_ids = {ann['image_id'] for ann in filtered_annotations}\n",
    "\n",
    "    # Filter images based on these image IDs\n",
    "    filtered_images = [img for img in coco_data['images'] if img['id'] in image_ids]\n",
    "else:\n",
    "    # Use all images and annotations if no classes are specified\n",
    "    filtered_images = coco_data['images']\n",
    "    filtered_annotations = coco_data['annotations']\n",
    "\n",
    "# Shuffle the filtered images randomly\n",
    "random.shuffle(filtered_images)\n",
    "\n",
    "# Limit the number of images to the specified value\n",
    "val_images = filtered_images[:num_images]\n",
    "\n",
    "# Filter annotations for the selected images\n",
    "def filter_annotations(images, annotations):\n",
    "    image_ids = {img['id'] for img in images}\n",
    "    return [ann for ann in annotations if ann['image_id'] in image_ids]\n",
    "\n",
    "val_annotations = filter_annotations(val_images, filtered_annotations)\n",
    "\n",
    "# Create the new JSON structure for validation\n",
    "val_data = {\n",
    "    'images': val_images,\n",
    "    'annotations': val_annotations,\n",
    "    'categories': coco_data['categories']\n",
    "}\n",
    "\n",
    "# Save the new JSON file\n",
    "output_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_val2017-2k-5c.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(val_data, f)\n",
    "\n",
    "print(f\"Validation dataset created with {len(val_images)} images.\")\n",
    "if selected_classes:\n",
    "    print(f\"Filtered by classes: {selected_classes}\")\n",
    "else:\n",
    "    print(\"All classes included.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW\n",
    "import json, random, os\n",
    "\n",
    "src = '/nas.dbms/asera/PROJECTS/DATASET/COCO/annotations/instances_val2017.json'\n",
    "dst = '/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_val2017-870-5c.json'\n",
    "\n",
    "num_images = 2000\n",
    "keep_names = ['bicycle', 'car', 'motorcycle', 'bus', 'truck']\n",
    "\n",
    "data = json.load(open(src))\n",
    "keep_cats = [c for c in data['categories'] if c['name'] in keep_names]\n",
    "keep_cats = sorted(keep_cats, key=lambda c: keep_names.index(c['name']))\n",
    "old2new = {c['id']: i+1 for i, c in enumerate(keep_cats)}\n",
    "new_categories = [\n",
    "    {'id': i+1, 'name': c['name'], 'supercategory': c.get('supercategory','none')}\n",
    "    for i, c in enumerate(keep_cats)\n",
    "]\n",
    "\n",
    "anns = [a for a in data['annotations'] if a['category_id'] in old2new]\n",
    "for a in anns:\n",
    "    a['category_id'] = old2new[a['category_id']]\n",
    "\n",
    "keep_img_ids = {a['image_id'] for a in anns}\n",
    "imgs = [im for im in data['images'] if im['id'] in keep_img_ids]\n",
    "\n",
    "random.shuffle(imgs)\n",
    "imgs = imgs[:num_images]\n",
    "img_set = {im['id'] for im in imgs}\n",
    "anns = [a for a in anns if a['image_id'] in img_set]\n",
    "\n",
    "out = {\n",
    "    'info': {'description': 'COCO-5 val subset', 'version': '1.0'},\n",
    "    'images': imgs,\n",
    "    'annotations': anns,\n",
    "    'categories': new_categories\n",
    "}\n",
    "json.dump(out, open(dst,'w'))\n",
    "print(f'wrote {dst}: {len(imgs)} images, {len(anns)} anns, {len(new_categories)} cats')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 1 Script for Splitting the Dataset to Training and Validation from a one source\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Path to the original COCO annotations\n",
    "original_ann_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/instances_val2017.json'\n",
    "\n",
    "# Load the original annotations\n",
    "with open(original_ann_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Shuffle the images randomly\n",
    "random.shuffle(coco_data['images'])\n",
    "\n",
    "# Split 80% for training and 20% for validation\n",
    "split_idx = int(0.8 * len(coco_data['images']))\n",
    "train_images = coco_data['images'][:split_idx]\n",
    "val_images = coco_data['images'][split_idx:]\n",
    "\n",
    "# Create a helper function to filter annotations by image IDs\n",
    "def filter_annotations(images, annotations):\n",
    "    image_ids = {img['id'] for img in images}\n",
    "    return [ann for ann in annotations if ann['image_id'] in image_ids]\n",
    "\n",
    "# Filter annotations for the new splits\n",
    "train_annotations = filter_annotations(train_images, coco_data['annotations'])\n",
    "val_annotations = filter_annotations(val_images, coco_data['annotations'])\n",
    "\n",
    "# Create the new JSON structures\n",
    "train_data = {'images': train_images, 'annotations': train_annotations, 'categories': coco_data['categories']}\n",
    "val_data = {'images': val_images, 'annotations': val_annotations, 'categories': coco_data['categories']}\n",
    "\n",
    "# Save the new JSON files\n",
    "with open('/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_train2017.json', 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open('/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_val2017.json', 'w') as f:\n",
    "    json.dump(val_data, f)\n",
    "\n",
    "print(f\"Training images: {len(train_images)}, Validation images: {len(val_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 2 Script for Splitting the Dataset to Training and Validation from a one source\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths for the original COCO annotations and images\n",
    "original_ann_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/annotations/instances_train2017.json'\n",
    "original_image_folder = '/nas.dbms/asera/PROJECTS/DATASET/COCO/images/train2017'\n",
    "mini_image_folder = '/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-images/val2017-2'\n",
    "\n",
    "# Ensure the mini dataset image folder exists\n",
    "os.makedirs(mini_image_folder, exist_ok=True)\n",
    "\n",
    "# Load the original annotations\n",
    "with open(original_ann_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Parameters for the mini dataset\n",
    "images_total = 5000  # Total images in mini dataset (optional)\n",
    "images_per_class = None  # Number of images per class (optional)\n",
    "selected_classes = None  # Set to None for all classes or specify class IDs, e.g., [1, 3, 6]\n",
    "train_split = 0.8  # Percentage of images for training (e.g., 0.8 for 80% train, 20% val)\n",
    "\n",
    "# Determine which classes to include\n",
    "if selected_classes is None:\n",
    "    selected_classes = [cat['id'] for cat in coco_data['categories']]\n",
    "else:\n",
    "    # Validate that selected_classes only includes valid COCO category IDs\n",
    "    all_class_ids = {cat['id'] for cat in coco_data['categories']}\n",
    "    selected_classes = [cls for cls in selected_classes if cls in all_class_ids]\n",
    "\n",
    "# Organize annotations by class\n",
    "annotations_by_class = defaultdict(list)\n",
    "for ann in coco_data['annotations']:\n",
    "    if ann['category_id'] in selected_classes:\n",
    "        annotations_by_class[ann['category_id']].append(ann)\n",
    "\n",
    "# Determine images_per_class if not specified\n",
    "if images_per_class is None:\n",
    "    # Calculate images_per_class based on images_total and number of classes\n",
    "    total_classes = len(selected_classes)\n",
    "    images_per_class = (images_total // (2 * total_classes))  # Divided by 2 for equal train/val split\n",
    "\n",
    "    if images_per_class == 0:\n",
    "        images_per_class = 1  # Ensure at least one image per class per split\n",
    "\n",
    "# Collect images and annotations, ensuring an even split per class\n",
    "selected_image_ids = set()\n",
    "train_annotations = []\n",
    "val_annotations = []\n",
    "\n",
    "for class_id in selected_classes:\n",
    "    class_annotations = annotations_by_class[class_id]\n",
    "\n",
    "    # Shuffle and select annotations for this class\n",
    "    random.shuffle(class_annotations)\n",
    "    required_annotations = 2 * images_per_class  # Total needed for both splits\n",
    "\n",
    "    # Ensure we don't select more annotations than available\n",
    "    class_annotations = class_annotations[:required_annotations]\n",
    "\n",
    "    # Split into training and validation\n",
    "    train_class_annotations = class_annotations[:images_per_class]\n",
    "    val_class_annotations = class_annotations[images_per_class:2 * images_per_class]\n",
    "\n",
    "    # Collect image IDs and annotations for this class\n",
    "    for ann in train_class_annotations:\n",
    "        selected_image_ids.add(ann['image_id'])\n",
    "        train_annotations.append(ann)\n",
    "    for ann in val_class_annotations:\n",
    "        selected_image_ids.add(ann['image_id'])\n",
    "        val_annotations.append(ann)\n",
    "\n",
    "# Filter images based on selected IDs\n",
    "selected_images = [img for img in coco_data['images'] if img['id'] in selected_image_ids]\n",
    "\n",
    "# Separate images into train and validation sets\n",
    "train_image_ids = {ann['image_id'] for ann in train_annotations}\n",
    "val_image_ids = {ann['image_id'] for ann in val_annotations}\n",
    "\n",
    "train_images = [img for img in selected_images if img['id'] in train_image_ids]\n",
    "val_images = [img for img in selected_images if img['id'] in val_image_ids]\n",
    "\n",
    "# Create JSON structures for train and validation datasets\n",
    "filtered_categories = [cat for cat in coco_data['categories'] if cat['id'] in selected_classes]\n",
    "\n",
    "train_data = {\n",
    "    'images': train_images,\n",
    "    'annotations': train_annotations,\n",
    "    'categories': filtered_categories\n",
    "}\n",
    "\n",
    "val_data = {\n",
    "    'images': val_images,\n",
    "    'annotations': val_annotations,\n",
    "    'categories': filtered_categories\n",
    "}\n",
    "\n",
    "# Save the JSON files for the balanced mini dataset\n",
    "with open('/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_train2017-2.json', 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open('/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_val2017-2.json', 'w') as f:\n",
    "    json.dump(val_data, f)\n",
    "\n",
    "# Copy selected images to the mini dataset folder\n",
    "for img_info in selected_images:\n",
    "    img_filename = img_info['file_name']\n",
    "    src_path = os.path.join(original_image_folder, img_filename)\n",
    "    dest_path = os.path.join(mini_image_folder, img_filename)\n",
    "    shutil.copyfile(src_path, dest_path)\n",
    "\n",
    "print(f\"Balanced mini dataset created with:\")\n",
    "print(f\" - Training images: {len(train_images)}\")\n",
    "print(f\" - Validation images: {len(val_images)}\")\n",
    "print(f\" - Total images: {len(selected_images)}\")\n",
    "print(f\" - Classes included: {[cat['name'] for cat in filtered_categories]}\")\n",
    "print(f\"Each split has {images_per_class} images per class.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 3 Script for Splitting the Dataset to Training and Validation from a one source\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths for the original COCO annotations and images\n",
    "original_ann_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/annotations/instances_train2017.json'\n",
    "original_image_folder = '/nas.dbms/asera/PROJECTS/DATASET/COCO/images/train2017'\n",
    "mini_image_folder = '/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-images/val2017-25'\n",
    "\n",
    "# Ensure the mini dataset image folder exists\n",
    "os.makedirs(mini_image_folder, exist_ok=True)\n",
    "\n",
    "# Load the original annotations\n",
    "with open(original_ann_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Parameters for the mini dataset\n",
    "images_total = 25000  # Total images in mini dataset\n",
    "images_per_class = None  # Specify exact number of images per class, e.g., 100, or leave as None to balance based on images_total\n",
    "selected_classes = None  # Set to None for all classes or specify class IDs, e.g., [1, 3, 6]\n",
    "train_split = 0.8  # Percentage of images for training (80%) and validation (20%)\n",
    "\n",
    "# Determine which classes to include\n",
    "if selected_classes is None:\n",
    "    selected_classes = [cat['id'] for cat in coco_data['categories']]\n",
    "else:\n",
    "    # Validate that selected_classes only includes valid COCO category IDs\n",
    "    all_class_ids = {cat['id'] for cat in coco_data['categories']}\n",
    "    selected_classes = [cls for cls in selected_classes if cls in all_class_ids]\n",
    "\n",
    "# Calculate how many images per class based on `images_total` if `images_per_class` is not set\n",
    "if images_per_class is None:\n",
    "    images_per_class = images_total // len(selected_classes)\n",
    "\n",
    "# Organize annotations by class\n",
    "annotations_by_class = defaultdict(list)\n",
    "for ann in coco_data['annotations']:\n",
    "    if ann['category_id'] in selected_classes:\n",
    "        annotations_by_class[ann['category_id']].append(ann)\n",
    "\n",
    "# Collect images and annotations in a balanced way across classes\n",
    "selected_image_ids = set()\n",
    "selected_annotations = []\n",
    "for class_id in selected_classes:\n",
    "    class_annotations = annotations_by_class[class_id]\n",
    "    \n",
    "    # Shuffle and select the specified number of images for this class\n",
    "    random.shuffle(class_annotations)\n",
    "    class_annotations = class_annotations[:images_per_class]\n",
    "    \n",
    "    # Collect image IDs and annotations for this class\n",
    "    for ann in class_annotations:\n",
    "        selected_image_ids.add(ann['image_id'])\n",
    "        selected_annotations.append(ann)\n",
    "\n",
    "# Filter images based on selected IDs\n",
    "selected_images = [img for img in coco_data['images'] if img['id'] in selected_image_ids]\n",
    "\n",
    "# Shuffle selected images and split into train and validation sets\n",
    "random.shuffle(selected_images)\n",
    "split_idx = int(train_split * len(selected_images))\n",
    "train_images = selected_images[:split_idx]\n",
    "val_images = selected_images[split_idx:]\n",
    "\n",
    "# Filter annotations by train and validation image IDs\n",
    "def filter_annotations(images, annotations):\n",
    "    image_ids = {img['id'] for img in images}\n",
    "    return [ann for ann in annotations if ann['image_id'] in image_ids]\n",
    "\n",
    "train_annotations = filter_annotations(train_images, selected_annotations)\n",
    "val_annotations = filter_annotations(val_images, selected_annotations)\n",
    "\n",
    "# Create JSON structures for train and validation datasets\n",
    "train_data = {'images': train_images, 'annotations': train_annotations, 'categories': coco_data['categories']}\n",
    "val_data = {'images': val_images, 'annotations': val_annotations, 'categories': coco_data['categories']}\n",
    "\n",
    "# Save the JSON files for the balanced mini dataset\n",
    "with open('/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_train2017-25.json', 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open('/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_val2017-25.json', 'w') as f:\n",
    "    json.dump(val_data, f)\n",
    "\n",
    "# Copy selected images to the mini dataset folder\n",
    "for img_info in selected_images:\n",
    "    img_filename = img_info['file_name']\n",
    "    src_path = os.path.join(original_image_folder, img_filename)\n",
    "    dest_path = os.path.join(mini_image_folder, img_filename)\n",
    "    shutil.copyfile(src_path, dest_path)\n",
    "\n",
    "print(f\"Balanced mini dataset created with {len(train_images)} training images and {len(val_images)} validation images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 4 Script for Splitting the Dataset to Training and Validation from a one source\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths for the original COCO annotations and images\n",
    "original_ann_file = '/nas.dbms/asera/PROJECTS/DATASET/COCO/annotations/instances_train2017.json'\n",
    "original_image_folder = '/nas.dbms/asera/PROJECTS/DATASET/COCO/images/train2017'\n",
    "mini_image_folder = '/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-images/val2017-2'\n",
    "\n",
    "# Ensure the mini dataset image folder exists\n",
    "os.makedirs(mini_image_folder, exist_ok=True)\n",
    "\n",
    "# Load the original annotations\n",
    "with open(original_ann_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Parameters for the mini dataset\n",
    "images_total = 5000           # Total images in mini dataset\n",
    "train_split = 0.8             # Percentage of images for training\n",
    "selected_classes = None #[1, 2, 3, 4, 6, 8]      # None for all classes, or specify class IDs, e.g., [1, 3, 6]\n",
    "\n",
    "# Determine which classes to include\n",
    "if selected_classes is None:\n",
    "    selected_classes = [cat['id'] for cat in coco_data['categories']]\n",
    "else:\n",
    "    # Validate that selected_classes only includes valid COCO category IDs\n",
    "    all_class_ids = {cat['id'] for cat in coco_data['categories']}\n",
    "    selected_classes = [cls for cls in selected_classes if cls in all_class_ids]\n",
    "\n",
    "# Organize annotations by class\n",
    "annotations_by_class = defaultdict(list)\n",
    "for ann in coco_data['annotations']:\n",
    "    if ann['category_id'] in selected_classes:\n",
    "        annotations_by_class[ann['category_id']].append(ann)\n",
    "\n",
    "# Calculate images per class for each split\n",
    "num_classes = len(selected_classes)\n",
    "total_images_per_class = images_total // num_classes\n",
    "images_per_class_train = int(total_images_per_class * train_split)\n",
    "images_per_class_val = total_images_per_class - images_per_class_train\n",
    "\n",
    "# Collect images and annotations, ensuring equal distribution for each class in both splits\n",
    "selected_image_ids = set()\n",
    "train_annotations = []\n",
    "val_annotations = []\n",
    "\n",
    "for class_id in selected_classes:\n",
    "    class_annotations = annotations_by_class[class_id]\n",
    "\n",
    "    # Shuffle and select the required number of annotations for this class\n",
    "    random.shuffle(class_annotations)\n",
    "    required_annotations = images_per_class_train + images_per_class_val\n",
    "\n",
    "    # Ensure we don't select more annotations than available\n",
    "    class_annotations = class_annotations[:required_annotations]\n",
    "\n",
    "    # Split into training and validation\n",
    "    train_class_annotations = class_annotations[:images_per_class_train]\n",
    "    val_class_annotations = class_annotations[images_per_class_train:required_annotations]\n",
    "\n",
    "    # Collect image IDs and annotations for this class\n",
    "    for ann in train_class_annotations:\n",
    "        selected_image_ids.add(ann['image_id'])\n",
    "        train_annotations.append(ann)\n",
    "    for ann in val_class_annotations:\n",
    "        selected_image_ids.add(ann['image_id'])\n",
    "        val_annotations.append(ann)\n",
    "\n",
    "# Filter images based on selected IDs\n",
    "selected_images = [img for img in coco_data['images'] if img['id'] in selected_image_ids]\n",
    "\n",
    "# Separate images into train and validation sets\n",
    "train_image_ids = {ann['image_id'] for ann in train_annotations}\n",
    "val_image_ids = {ann['image_id'] for ann in val_annotations}\n",
    "\n",
    "train_images = [img for img in selected_images if img['id'] in train_image_ids]\n",
    "val_images = [img for img in selected_images if img['id'] in val_image_ids]\n",
    "\n",
    "# Create JSON structures for train and validation datasets\n",
    "filtered_categories = [cat for cat in coco_data['categories'] if cat['id'] in selected_classes]\n",
    "\n",
    "train_data = {\n",
    "    'images': train_images,\n",
    "    'annotations': train_annotations,\n",
    "    'categories': filtered_categories\n",
    "}\n",
    "\n",
    "val_data = {\n",
    "    'images': val_images,\n",
    "    'annotations': val_annotations,\n",
    "    'categories': filtered_categories\n",
    "}\n",
    "\n",
    "# Save the JSON files for the balanced mini dataset\n",
    "with open('/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_train2017-2.json', 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open('/nas.dbms/asera/PROJECTS/DATASET/COCO/mini-annotations/mini_val2017-2.json', 'w') as f:\n",
    "    json.dump(val_data, f)\n",
    "\n",
    "# Copy selected images to the mini dataset folder\n",
    "for img_info in selected_images:\n",
    "    img_filename = img_info['file_name']\n",
    "    src_path = os.path.join(original_image_folder, img_filename)\n",
    "    dest_path = os.path.join(mini_image_folder, img_filename)\n",
    "    shutil.copyfile(src_path, dest_path)\n",
    "\n",
    "print(f\"Balanced mini dataset created with:\")\n",
    "print(f\" - Training images: {len(train_images)}\")\n",
    "print(f\" - Validation images: {len(val_images)}\")\n",
    "print(f\" - Total images: {len(selected_images)}\")\n",
    "print(f\" - Classes included: {[cat['name'] for cat in filtered_categories]}\")\n",
    "print(f\"Each class has {images_per_class_train} images in training and {images_per_class_val} images in validation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA_11.6_env-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
